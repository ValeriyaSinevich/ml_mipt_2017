{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "word = []\n",
    "inf = []\n",
    "particle = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('task2_lemmas_train.txt') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        line = line.split(',')\n",
    "        word.append(line[1])\n",
    "        line = line[2].split('+')   \n",
    "        inf.append(line[0])\n",
    "        particle.append(line[1][0])\n",
    "        \n",
    "particle = list(map(lambda x: 0 if x == 'V' else (1 if x == 'A' else 2), particle))\n",
    "data['word'] = word\n",
    "data['infinitiv'] = inf\n",
    "data['particle'] = particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_end(word, stem):\n",
    "    return word[re.search(stem, word).span()[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_end = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inf_end = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    suf.append(get_suf(data['word_end'][i], data['inf_end'][i], data['word_stem'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    try:\n",
    "        word_end.append(get_end(data['word'][i], data['word_stem'][i]))\n",
    "    except:\n",
    "        word_end.append(data['word'][i])\n",
    "        continue\n",
    "        print(get_end(data['word'][i], data['word_stem'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    try:\n",
    "        inf_end.append(get_end(data['infinitiv'][i], data['word_stem'][i]))\n",
    "    except:\n",
    "        inf_end.append(data['infinitiv'][i])\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_suf(a, b, stem):\n",
    "    for i in range(min(len(a), len(b))):\n",
    "        if a[i] != b[i]:\n",
    "            return a[0:i]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['word_end'] = word_end\n",
    "data[\"inf_end\"] = inf_end\n",
    "data['suf'] = suf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def longest_common(infinitiv, *args, **kwargs):\n",
    "    args = list(args)\n",
    "    args.append(infinitiv)\n",
    "    if len(set(map(lambda x: x[0], args))) > 1:\n",
    "        first = {}\n",
    "        for arg in args:\n",
    "            if arg[0] in first.keys():\n",
    "                first[arg[0]] += 1\n",
    "            else:\n",
    "                first[arg[0]] = 1\n",
    "        first = sorted([(first[k], k) for k in list(first.keys())])\n",
    "        start_letter = first[-1][1]\n",
    "        args = list(filter(lambda x: x[0] == start_letter, args))\n",
    "    for i in range(1, len(args[0])):\n",
    "        try:\n",
    "            if len(set(map(lambda x: x[i], args))) > 1:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "    return args[0][0:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stem = {}\n",
    "for word in words:\n",
    "    stem[word] = (longest_common(word, *data.query('infinitiv == \\'{}\\''.format(word))['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_stem = list(map(lambda x: stem[x],  data['infinitiv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['word_stem'] = word_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"lemmatization2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>infinitiv</th>\n",
       "      <th>particle</th>\n",
       "      <th>word_stem</th>\n",
       "      <th>suf</th>\n",
       "      <th>word_end</th>\n",
       "      <th>inf_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vergognerete</td>\n",
       "      <td>vergognare</td>\n",
       "      <td>0</td>\n",
       "      <td>vergogn</td>\n",
       "      <td></td>\n",
       "      <td>erete</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amnistiavate</td>\n",
       "      <td>amnistiare</td>\n",
       "      <td>0</td>\n",
       "      <td>amnisti</td>\n",
       "      <td>a</td>\n",
       "      <td>avate</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>menomazione</td>\n",
       "      <td>menomazione</td>\n",
       "      <td>2</td>\n",
       "      <td>menomazion</td>\n",
       "      <td></td>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sfaldavamo</td>\n",
       "      <td>sfaldare</td>\n",
       "      <td>0</td>\n",
       "      <td>sfald</td>\n",
       "      <td>a</td>\n",
       "      <td>avamo</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sfodererei</td>\n",
       "      <td>sfoderare</td>\n",
       "      <td>0</td>\n",
       "      <td>sfoder</td>\n",
       "      <td></td>\n",
       "      <td>erei</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word    infinitiv  particle   word_stem suf word_end inf_end\n",
       "0  vergognerete   vergognare         0     vergogn        erete     are\n",
       "1  amnistiavate   amnistiare         0     amnisti   a    avate     are\n",
       "2   menomazione  menomazione         2  menomazion            e       e\n",
       "3    sfaldavamo     sfaldare         0       sfald   a    avamo     are\n",
       "4    sfodererei    sfoderare         0      sfoder         erei     are"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2025"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_end = set(data['word_end'])\n",
    "len(possible_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_suf = set(data['suf'])\n",
    "len(possible_suf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_inf_end = set(data['inf_end'])\n",
    "len(possible_inf_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_data = copy.deepcopy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_end_num = {list(possible_end)[i] : i for i in range(len(list(possible_end)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_word_end(data):\n",
    "    attrs = np.zeros((len(possible_end), data.shape[0]))\n",
    "    for i, end in enumerate(data['word_end']):\n",
    "        attrs[possible_end_num[end]][i] = 1\n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attrs = process_word_end(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(attrs.T[i].sum() == 1 for i in range(len(attrs.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, attr in enumerate(attrs):\n",
    "    processed_data['f{}'.format(i)] = attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>infinitiv</th>\n",
       "      <th>particle</th>\n",
       "      <th>word_stem</th>\n",
       "      <th>suf</th>\n",
       "      <th>word_end</th>\n",
       "      <th>inf_end</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>...</th>\n",
       "      <th>f2015</th>\n",
       "      <th>f2016</th>\n",
       "      <th>f2017</th>\n",
       "      <th>f2018</th>\n",
       "      <th>f2019</th>\n",
       "      <th>f2020</th>\n",
       "      <th>f2021</th>\n",
       "      <th>f2022</th>\n",
       "      <th>f2023</th>\n",
       "      <th>f2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vergognerete</td>\n",
       "      <td>vergognare</td>\n",
       "      <td>0</td>\n",
       "      <td>vergogn</td>\n",
       "      <td></td>\n",
       "      <td>erete</td>\n",
       "      <td>are</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amnistiavate</td>\n",
       "      <td>amnistiare</td>\n",
       "      <td>0</td>\n",
       "      <td>amnisti</td>\n",
       "      <td>a</td>\n",
       "      <td>avate</td>\n",
       "      <td>are</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>menomazione</td>\n",
       "      <td>menomazione</td>\n",
       "      <td>2</td>\n",
       "      <td>menomazion</td>\n",
       "      <td></td>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sfaldavamo</td>\n",
       "      <td>sfaldare</td>\n",
       "      <td>0</td>\n",
       "      <td>sfald</td>\n",
       "      <td>a</td>\n",
       "      <td>avamo</td>\n",
       "      <td>are</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sfodererei</td>\n",
       "      <td>sfoderare</td>\n",
       "      <td>0</td>\n",
       "      <td>sfoder</td>\n",
       "      <td></td>\n",
       "      <td>erei</td>\n",
       "      <td>are</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2032 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word    infinitiv  particle   word_stem suf word_end inf_end   f0  \\\n",
       "0  vergognerete   vergognare         0     vergogn        erete     are  0.0   \n",
       "1  amnistiavate   amnistiare         0     amnisti   a    avate     are  0.0   \n",
       "2   menomazione  menomazione         2  menomazion            e       e  0.0   \n",
       "3    sfaldavamo     sfaldare         0       sfald   a    avamo     are  0.0   \n",
       "4    sfodererei    sfoderare         0      sfoder         erei     are  0.0   \n",
       "\n",
       "    f1   f2  ...    f2015  f2016  f2017  f2018  f2019  f2020  f2021  f2022  \\\n",
       "0  0.0  0.0  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1  0.0  0.0  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2  0.0  0.0  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3  0.0  0.0  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4  0.0  0.0  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   f2023  f2024  \n",
       "0    0.0    0.0  \n",
       "1    0.0    0.0  \n",
       "2    0.0    0.0  \n",
       "3    0.0    0.0  \n",
       "4    0.0    0.0  \n",
       "\n",
       "[5 rows x 2032 columns]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_data.to_csv(\"lemmatization_P.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_inf_end= (list(possible_inf_end))\n",
    "def process_class():\n",
    "    possible_inf_end_num = {list(possible_inf_end)[i] : i for i in range(len((possible_inf_end)))}\n",
    "    return possible_inf_end_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = process_class()\n",
    "result = list(map(lambda x: classes[x], data['inf_end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['result_end'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_end(word):\n",
    "    return sorted(list(filter(lambda x: word.endswith(x), possible_end)), key=lambda x: len(x))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame()\n",
    "test_word = []\n",
    "with open('task2_lemmas_test.txt') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        test_word.append(line.split(\"\\n\")[0].split(',')[1])\n",
    "        \n",
    "test['word'] = test_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_end = list(map(lambda x: get_test_end(x), test['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['word_end'] = test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_attrs = process_word_end(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(test_attrs.T[i].sum() == 1 for i in range(len(test_attrs.T)))\n",
    "assert(len(test_attrs) == len(attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_test = copy.deepcopy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, attr in enumerate(test_attrs):\n",
    "    processed_test['f{}'.format(i)] = attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_end</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>...</th>\n",
       "      <th>f2015</th>\n",
       "      <th>f2016</th>\n",
       "      <th>f2017</th>\n",
       "      <th>f2018</th>\n",
       "      <th>f2019</th>\n",
       "      <th>f2020</th>\n",
       "      <th>f2021</th>\n",
       "      <th>f2022</th>\n",
       "      <th>f2023</th>\n",
       "      <th>f2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gettonan</td>\n",
       "      <td>an</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incidentali</td>\n",
       "      <td>li</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>involtino</td>\n",
       "      <td>ino</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lievi</td>\n",
       "      <td>evi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comunistizzasse</td>\n",
       "      <td>asse</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2027 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word word_end   f0   f1   f2   f3   f4   f5   f6   f7  ...    \\\n",
       "0         gettonan       an  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     \n",
       "1      incidentali       li  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     \n",
       "2        involtino      ino  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     \n",
       "3            lievi      evi  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     \n",
       "4  comunistizzasse     asse  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     \n",
       "\n",
       "   f2015  f2016  f2017  f2018  f2019  f2020  f2021  f2022  f2023  f2024  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 2027 columns]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_test.to_csv(\"lemmatisation_test_P.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1\n",
    "clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1.fit(data.drop('suf', 'inf_end', 'particle'), data['particle'])\n",
    "clf2.fit(data.drop('suf', 'inf_end'), data['inf_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_infinitiv(test):\n",
    "    test_res = clf2.predict(test.drop('word', \"word_end\"))\n",
    "    main_res = []\n",
    "    for i, word in enumerate(test['word']):\n",
    "        main_res.append(word[:len(word) - len(test['word_end'][i])] + classes[test_res[i]])\n",
    "    return main_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_partice(test):\n",
    "    result =  clf1.predict(test.drop('word', \"word_end\"))\n",
    "    processed_test['particle'] = result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
